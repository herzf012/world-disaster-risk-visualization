{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b3925",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "#API key pulled from a config.py in format of `prog_search_key` =  \"your_key_here\" \n",
    "#from config import prog_search_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78be8b",
   "metadata": {},
   "source": [
    "# Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761456e7",
   "metadata": {},
   "source": [
    "### <center> Attempt using country_codes_combined.csv </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce1a4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "country_codes_csv = pd.read_csv('static/data/country_codes_combined.csv')\n",
    "country_codes_csv = country_codes_csv[ ['alpha2','de','en'] ]\n",
    "country_codes_df = country_codes_csv.copy()\n",
    "country_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f720e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "world_risk_index_csv = pd.read_csv('static/data/world_risk_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da710035",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Found during transformation was this mis-input row. It must be manually adjusted or deleted.\n",
    "world_risk_index_csv.loc[[1858]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b95152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Because the entire index of 1858 was misinput I manually adjust the row across each column. Parameters for \n",
    "## Category found from supporting document from data creators.\n",
    "\n",
    "korea_row_fix = ['Korea, Republic of', 4.59, 14.89, 30.82, 14.31, 46.55, 31.59,2016,'High','Low','Very Low','Very Low']\n",
    "\n",
    "world_risk_index_csv.loc[[1858]] = korea_row_fix\n",
    "world_risk_index_csv.loc[[1858]] = world_risk_index_csv.loc[[1858]].copy()\n",
    "world_risk_index_csv.loc[[1858]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ccddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get length of index as a reference to ensure consistent dataframe size through-out. \n",
    "\n",
    "print(f'Rows in country_codes_df: {len(country_codes_df.index)}')\n",
    "print(f'Rows in country_codes_df: {len(world_risk_index_csv.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232bfef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Making two dataframes based on imported English and German country codes & region names. These two dataframes are then\n",
    "## merged together into one dataframe so there is an English reference point to the German region names.\n",
    "\n",
    "merged_df_en = world_risk_index_csv.merge(country_codes_df, how='left', left_on='Region', right_on='en')\n",
    "merged_df_de = world_risk_index_csv.merge(country_codes_df, how='left', left_on='Region', right_on='de')\n",
    "merged_all = pd.concat([merged_df_en, merged_df_de])\n",
    "merged_dropped = merged_all.dropna().copy()\n",
    "merged_world = merged_dropped.merge(world_risk_index_csv,how='right')\n",
    "merged_final = merged_world.drop_duplicates(ignore_index=True).copy()\n",
    "merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87c54a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A check to see if there are any Region inputs that do not have a corresponding alpha2 code, which is the \n",
    "## consistent method used for our translation. The German region name and the English region name will always share the\n",
    "### same unified country code. \n",
    "\n",
    "merged_final_nulls = merged_final[merged_final['alpha2'].isnull()]\n",
    "merged_final_nulls_drop = merged_final_nulls.drop_duplicates(subset=['Region']).copy()\n",
    "\n",
    "print(f' Length of null values without duplicates removed: {len(merged_final_nulls.index)}')\n",
    "print(f' Length of null values with duplicates removed: {len(merged_final_nulls_drop.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c73c7",
   "metadata": {},
   "source": [
    "###  <center> Attempt at Pulling From Additional Source </center>\n",
    "\n",
    "As there are 130 rows that show up with lack of information from the first source two additional websites were pulled from, one in English and one in German. The desire was that this would provide coverage that may have been missed from the original `country_codes_combined.csv` \n",
    "    \n",
    "English Source: [Cloford.com](https://cloford.com/resources/codes/index.htm)\n",
    "    \n",
    "German Source: [oenb.at](https://www.oenb.at/Statistik/Klassifikationen/ISO-Codes/ISO-Code-Verzeichnis-fuer-Laender--und-Waehrungscodes.html)    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eac314",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code to pull tables from English [country:country_code] code source\n",
    "\n",
    "url = \"https://cloford.com/resources/codes/index.htm\"\n",
    "\n",
    "country_code_import = pd.read_html(url)\n",
    "country_code_draft = country_code_import[3].copy()\n",
    "country_code_df = country_code_draft[  ['Country','ISO (2)','Continent','Region','Capital' ]  ]\n",
    "country_code_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4fb5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code to pull tables from German [country:country_code] code source\n",
    "\n",
    "ger_url = \"https://www.oenb.at/Statistik/Klassifikationen/ISO-Codes/ISO-Code-Verzeichnis-fuer-Laender--und-Waehrungscodes.html\"\n",
    "ger_codes = pd.read_html(ger_url)\n",
    "ger_code_draft = ger_codes[0].copy()\n",
    "ger_code_df = ger_code_draft[ ['Land','ISO-Code (Land)'] ]\n",
    "ger_code_df = ger_code_df.fillna(\"\").copy()\n",
    "ger_code_df_clean =  ger_code_df.loc[ger_code_df['ISO-Code (Land)']!='一一一']\n",
    "ger_code_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52889a4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge both German and English external sources to apply to primary dataset.\n",
    "\n",
    "merged_import_codes = ger_code_df_clean.merge(country_code_df, left_on='ISO-Code (Land)', right_on='ISO (2)').copy()\n",
    "merged_import_codes_rename= merged_import_codes.rename(columns={'Region':'Area'}).copy()\n",
    "merged_import_codes_rename.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21b8ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge combined external resource dataframes with the primary dataset\n",
    "\n",
    "merged_df_import_de = world_risk_index_csv.merge(merged_import_codes_rename, how='left', left_on='Region', right_on='Land')\n",
    "merged_df_import_en = world_risk_index_csv.merge(merged_import_codes_rename, how='left', left_on='Region', right_on='Country')\n",
    "import_merged_all = pd.concat([merged_df_import_en, merged_df_import_de])\n",
    "import_merged_dropped = import_merged_all.dropna().copy()\n",
    "import_merged_world = import_merged_dropped.merge(world_risk_index_csv,how='right')\n",
    "import_merged_final = import_merged_world.drop_duplicates(ignore_index=True).copy()\n",
    "import_merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ffb48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Determine how much coverage was provided by external resource\n",
    "\n",
    "import_final_nulls = import_merged_final[import_merged_final['ISO (2)'].isnull()]\n",
    "import_final_nulls_drop = import_final_nulls.drop_duplicates(subset=['Region']).copy()\n",
    "\n",
    "print(f' Length of null values without duplicates removed: {len(import_final_nulls.index)}')\n",
    "print(f' Length of null values with duplicates removed: {len(import_final_nulls_drop.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d04df",
   "metadata": {},
   "source": [
    "### <center> Combine csv source dataframe with externally pulled source dataframe </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920776f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import_csv_merge = import_merged_final.merge(merged_final,how='outer').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff5b1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create dataframe out of both ISO code fields from each source.\n",
    "iso_codes_df = import_csv_merge[ ['ISO (2)','alpha2'] ]\n",
    "\n",
    "## Where ISO 2 is null I want to find an alpha2 row that is not null so that I can make sure that the backfill I use in the\n",
    "### next cell works properly. \n",
    "iso_codes_df[iso_codes_df['ISO (2)'].isnull()].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef58740",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import_csv_merge['iso_code'] = iso_codes_df.bfill(axis=1).iloc[:, 0]\n",
    "import_csv_merge.loc[import_csv_merge['iso_code'] == \"tl\"].head(2)\n",
    "\n",
    "# The bfill works properly. Where there is a NULL in ISO (2) iso_code and alpha2 are filled. Performing the inverse\n",
    "## will confirm that everything backfilled properly and we have the correct amount of leftover NULL codes that were not\n",
    "### filled at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc90e4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iso_codes_df[iso_codes_df['alpha2'].isnull()].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981e589",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import_csv_merge['iso_code'] = iso_codes_df.bfill(axis=1).iloc[:, 0]\n",
    "import_csv_merge.loc[import_csv_merge['iso_code'] == \"CG\"].head(2)\n",
    "\n",
    "# As expected, where alpha2 is null, both iso_code and ISO (2) are not null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783b9d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_source_nulls = import_csv_merge[import_csv_merge['iso_code'].isnull()]\n",
    "merged_source_nulls_drop = merged_source_nulls.drop_duplicates(subset=['Region']).copy()\n",
    "\n",
    "print(f' Length of null values without duplicates removed: {len(merged_source_nulls.index)}')\n",
    "print(f' Length of null values with duplicates removed: {len(merged_source_nulls_drop.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c2c39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e388a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# null_codes = import_csv_merge[import_csv_merge['iso_code'].isnull()].copy()\n",
    "# null_codes['Region'] = null_codes['Region'].drop_duplicates().copy()\n",
    "# null_codes = null_codes[null_codes['Region'].notna()]\n",
    "# # null_codes['Region'] = null_codes['Region'].str.replace('\\d+', '')\n",
    "# null_codes = null_codes.reset_index()\n",
    "# null_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25fee4",
   "metadata": {},
   "source": [
    "## Attempt to finish cleaning by using custom google search API\n",
    "\n",
    "Finally, after trying to use two different sources for screening German names I found out that the original data input from the original dataset source was inconsistent. Following is how I solved this problem using Google's Custom Search API. \n",
    "\n",
    "Fortunately I was able to cut the amount of inconsistent naming conventions down to 33 unique countries. This fits within the Custom Search API's 100 free daily search limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c5b36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_codes_list = []\n",
    "null_codes_list_comp = []\n",
    "null_codes_list = merged_source_nulls_drop['Region'].tolist()\n",
    "null_codes_list = [\n",
    "\"Vereinigte Staaten von Amerika\" if ('Vereinigte Staaten v. A.') in country else country for country in null_codes_list\n",
    "].copy()\n",
    "null_codes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e5ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Retroactively, after performing the first API call, these countries needed to be manually transformed in order to \n",
    "### utilize the API. A follow up API called was performed on the transformed named and the results came back correctly.\n",
    "### Since it has been confirmed to work, I have retroactively changed them before the first API call so that for any \n",
    "### additional use of this notebook only requires 1 API call. I have not removed the code for the 2nd API call. I have \n",
    "### simply commented it out. \n",
    "\n",
    "\n",
    "null_codes_list_comp = [\n",
    "    \"North Macedonia\" if country == 'T. f. Yugo. Rep. of Macedonia' else country for country in null_codes_list\n",
    "].copy()\n",
    "\n",
    "null_codes_list_comp = [\n",
    "    \"United States\" if ('Vereinigte Staaten') in country else country for country in null_codes_list_comp\n",
    "].copy()\n",
    "\n",
    "\n",
    "null_codes_list_comp = [\n",
    "    \"Central African Republic\" if ('Zentralafrik') in country else country for country in null_codes_list_comp\n",
    "].copy() \n",
    "\n",
    "null_codes_list_comp = [\n",
    "    \"United Arab Emirates\" if ('Arabische Emirate') in country else country for country in null_codes_list_comp\n",
    "].copy() \n",
    "\n",
    "null_codes_list_comp = [\n",
    "    \"Democratic Republic of the Congo\" if ('Kongo') in country else country for country in null_codes_list_comp\n",
    "].copy() \n",
    "\n",
    "null_codes_list_comp = [\n",
    "    \"Federated States of Micronesia\" if ('Mikronesien') in country else country for country in null_codes_list_comp\n",
    "].copy()\n",
    "\n",
    "null_codes_list_comp = [\n",
    "    \"Saint Vincent and the Grenadines\" if ('St. Vincent') in country else country for country in null_codes_list_comp\n",
    "].copy()\n",
    "null_codes_list_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6920d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf6e24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All of the manually changed regions as a dictionary. Necessary to match country codes after since the original inputs\n",
    "## are being changed from how they appear. \n",
    "\n",
    "changed_regions_dict = {\n",
    "    'English': [\n",
    "    \"North Macedonia\",\"United States\",\"United States\",\"United States\",\"Central African Republic\",\"United Arab Emirates\",\n",
    "    \"Democratic Republic of the Congo\",\"Federated States of Micronesia\",\"Federated States of Micronesia\",\n",
    "    \"Saint Vincent and the Grenadines\",\"Saint Vincent and the Grenadines\",\"Saint Vincent and the Grenadines\"],\n",
    "    'German':[\n",
    "        'T. f. Yugo. Rep. of Macedonia','Vereinigte Staaten v. A.','Vereinigte Staaten von Amerika',\n",
    "        'Ver. Staaten von Amerika','Zentralafrik. Republik','Ver. Arabische Emirate','Demokratische Rep. Kongo',\n",
    "        'Föd. Staaten von Mikronesien','Föd. Staaten v. Mikronesien','St. Vincent und d. Grenadinen',\n",
    "        'St. Vincent u. d. Grenadinen','St. Vincent u. die Grenadinen']\n",
    "}\n",
    "\n",
    "changed_regions_df = pd.DataFrame.from_dict(changed_regions_dict)\n",
    "changed_regions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b318f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "api_call_list = []\n",
    "[api_call_list.append(country) for country in null_codes_list_comp if country not in api_call_list].copy()\n",
    "api_call_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c1c3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# api_url = \"https://customsearch.googleapis.com/customsearch/v1?\"\n",
    "# cx = \"d3772df2249924485\"\n",
    "# key = prog_search_key\n",
    "# num = 1\n",
    "# site_search = \"https://en.wikipedia.org/wiki/ISO_3166-2:\"\n",
    "# search_filter = \"i\"\n",
    "# query_url = (f\"{api_url}cx={cx}&key={key}&num={num}&{site_search}&{search_filter}&q=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645b93e",
   "metadata": {},
   "source": [
    "# API Call \n",
    "## Please do not try to run this cell. I have set it to read-only. \n",
    "\n",
    "I have also commented it out as it should ONLY be used by Jacob McManaman, or by someone who knows what they are doing (or who is aware that *thinking* they know what they are doing can easily have consequences) and has willingly set up their Google API key for use with Google's Custom Search API. Someone who has done so must also have acknowledged that there is by default only 100 searches per day. Thoughtfully, this call will only run 33 searches.\n",
    "\n",
    "API aside, running this cell will reset the `request_list` list which *can* be something incredibly annoying. I believe I have taken steps to circumvent any accidents, but in the event I have not taken enough precaution, should someone go through the effort to change the cell from read-only and runs the cell frivolously, you will make the writer of this markdown doomingly sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae8e6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# request_list= []\n",
    "# for country in api_call_list:\n",
    "#     counter = counter + 1\n",
    "#     query = requests.get(f\"{query_url}{country} iso code\").json()\n",
    "#     print(f\"Search Request {counter} of {len(api_call_list)} : {country}\")\n",
    "#     request_list.append(query)\n",
    "#     time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f75045",
   "metadata": {},
   "source": [
    " ### Reasons for and Mechanics of the API Call:\n",
    "Originally, I had hoped that there was consistency with the original Dataset. I was very wrong and the German country/region names deviated from convention. Thankfully I was able to clean 98% (1884/1917) of the German region names using two external sources. \n",
    "\n",
    "I found that I could just google the final 2% (33) country names and google would correct the search to produce a country code provided from `de.wikipedia.org/`. A useful tool google provides is the ability to filter by website, through a **site:`www.example.com`** query, or by creating a [Programmable Search Engine](https://programmablesearchengine.google.com/about/). This programable enginge can then be utilized by [Google's Custom Search API](https://developers.google.com/custom-search/v1/overview). Limited by 100 free searches a day, this project is very fortunate that only 33 of the data needed this treatment. The overview of the API is as follows;\n",
    "\n",
    "`https://www.googleapis.com/customsearch/v1/siterestrict?cx=   &key=   &q=`\n",
    "\n",
    "Where `?cx=` is the engine ID that is referenced for the search, the `&key=` is the API key that is used to make the call, and `&q=` is the query. \n",
    "\n",
    "And so this API call utilizes a programmable engine set to specifically filter websites by `de.wikipedia.org/`. While other websites did populate, since a call needed to be made individually for each erroneous data , the german wikipedia was preferable since its results returned the single country/regions information page, while others returned a table with every other country code. Organically, the search would look something like this:\n",
    "\n",
    "![title](data/images/organic_search.png)\n",
    "\n",
    "Thanks to Google, any sort of cleaning of poorly inputed data is done for us by these request. It's just up to us to clean the resulting request results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeeed1a",
   "metadata": {},
   "source": [
    "## JSON cleaning\n",
    "\n",
    "Once the API call is done, the resulting JSON is sent to a list and that list is cleaned in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a60e5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#request_list[0]['items'][0]['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d95af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# request_url = request_list[0]['items'][0]['link']\n",
    "# split_list = request_url.split(':')\n",
    "# split_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a12b6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bad_requests = []\n",
    "# good_requests = []\n",
    "# all_country_codes = []\n",
    "# country_codes = []\n",
    "# for request in range(len(request_list)):\n",
    "#     try:\n",
    "#         request_url = request_list[request]['items'][0]['link']\n",
    "#         split_list = request_url.split(':')\n",
    "#         print(f\"Country Code: {split_list[2]}\")\n",
    "#         all_country_codes.append(split_list[2])\n",
    "#         country_codes.append(split_list[2])\n",
    "#         good_requests.append(request_list[request]['queries']['request'][0]['searchTerms'])\n",
    "#     except(KeyError):\n",
    "#         print(f\"Skipped request {request}: {request_list[request]['queries']['request'][0]['searchTerms']}\")\n",
    "#         bad_requests.append(request_list[request]['queries']['request'][0]['searchTerms'])\n",
    "#         all_country_codes.append(request_list[request]['queries']['request'][0]['searchTerms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0e005",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#change_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf765e38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# codes_country_df = pd.DataFrame({\n",
    "#     'iso_code': country_codes,\n",
    "#     'Regions': api_call_list\n",
    "# })\n",
    "# codes_country_df = codes_country_df.merge(changed_regions_df,how='left',left_on = 'Regions',right_on='English')\n",
    "# codes_country_df['German'] = codes_country_df['German'].fillna(codes_country_df['Regions'])\n",
    "# full_country_codes_df = codes_country_df[ ['German','iso_code'] ]\n",
    "# full_country_codes_df.to_csv('static/data/corrected_country_codes.csv',index=False)\n",
    "# full_country_codes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7765c8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Both of these lists should provide the same length of the API call and resulting cleaning of the JSON was successful.\n",
    "\n",
    "# print(f' Length of api_call_list: {len(api_call_list)}')\n",
    "# print(f' Length of full_country_codes_df: {len(full_country_codes_df.index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736464c2",
   "metadata": {},
   "source": [
    "# Second API Call \n",
    "\n",
    "No longer needed as explained above - was used to further refine the bad requests recieved from the first API call. All cells have been set to READ-ONLY to make sure they are not accidentally deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a77f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# fixed_request_list= []\n",
    "# for country in fixed_requests:\n",
    "#     counter = counter + 1\n",
    "#     query = requests.get(f\"{query_url}{country} iso code\").json()\n",
    "#     print(f\"Search Request {counter} of {len(fixed_requests)} : {country}\")\n",
    "#     fixed_request_list.append(query)\n",
    "#     time.sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7196e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test_bad_requests = []\n",
    "# test_all_country_codes = []\n",
    "# test_country_codes = []\n",
    "# for request in range(len(fixed_request_list)):\n",
    "#     try:\n",
    "#         fixed_test_url = fixed_request_list[request]['items'][0]['link']\n",
    "#         split_list = fixed_test_url.split(':')\n",
    "#         print(f\"Country Code: {split_list[2]}\")\n",
    "#         test_all_country_codes.append(split_list[2])\n",
    "#         test_country_codes.append(split_list[2])\n",
    "#     except(KeyError):\n",
    "#         print(f\"Skipped request {request}: {fixed_request_list[request]['queries']['request'][0]['searchTerms']}\")\n",
    "#         test_bad_requests.append(fixed_request_list[request]['queries']['request'][0]['searchTerms'])\n",
    "#         test_country_codes.append(fixed_request_list[request]['queries']['request'][0]['searchTerms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992e060",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iso_bad_request = []\n",
    "# iso_good_request = []\n",
    "\n",
    "# iso_bad_request = ([s.replace(' iso code', '') for s in bad_requests])\n",
    "\n",
    "# iso_good_request = ([s.replace(' iso code', '') for s in good_requests])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9e708",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fixed_bad_codes_df = pd.DataFrame({\n",
    "#     'alpha2': test_country_codes,\n",
    "#     'Regions': iso_bad_request\n",
    "# })\n",
    "\n",
    "# fixed_good_codes_df = pd.DataFrame({\n",
    "#     'alpha2': country_codes,\n",
    "#     'Regions': iso_good_request\n",
    "# })\n",
    "\n",
    "# joined_codes = fixed_bad_codes_df.merge(fixed_good_codes_df, how='right').copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddac59a",
   "metadata": {},
   "source": [
    "## Finish Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e4e35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_country_codes_csv = pd.read_csv('static/data/corrected_country_codes.csv')\n",
    "full_country_codes_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082664f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleaned_country_df = import_csv_merge.merge(full_country_codes_csv,how='left',left_on='Region',right_on='German').copy()\n",
    "cleaned_country_df['iso_code_y'] = cleaned_country_df['iso_code_y'].fillna(cleaned_country_df['iso_code_x'])\n",
    "cleaned_country_df = cleaned_country_df.drop(columns='iso_code_x')\n",
    "cleaned_country_df['iso_code_y'] = cleaned_country_df['iso_code_y'].fillna(cleaned_country_df['alpha2'])\n",
    "cleaned_country_df = cleaned_country_df.drop(columns='alpha2')\n",
    "cleaned_country_df['iso_code_y'] = cleaned_country_df['iso_code_y'].fillna(cleaned_country_df['ISO (2)'])\n",
    "cleaned_country_df = cleaned_country_df.drop(columns='ISO (2)')\n",
    "cleaned_country_df['iso_code_y'] = cleaned_country_df['iso_code_y'].fillna(cleaned_country_df['ISO-Code (Land)'])\n",
    "cleaned_country_df = cleaned_country_df.drop(columns=\n",
    "                                             ['ISO-Code (Land)','Land',\n",
    "                                             'Country','Continent','Area',\n",
    "                                             'Capital','de','en','German']\n",
    "                                            )\n",
    "cleaned_country_df[cleaned_country_df['iso_code_y'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36a3c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleaned_country_df = cleaned_country_df.rename(columns={'iso_code_y':'iso_code'})\n",
    "cleaned_country_df['iso_code'] = cleaned_country_df['iso_code'].str.upper()\n",
    "country_codes_df['alpha2'] = country_codes_df['alpha2'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0465bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataframe = cleaned_country_df.merge(country_codes_df,how='left',left_on='iso_code',right_on='alpha2')\n",
    "english_dataframe  = english_dataframe .drop(columns=\n",
    "                                             ['Region','alpha2',\n",
    "                                             'de'])\n",
    "\n",
    "english_dataframe = english_dataframe.sort_values(by = [\"Year\"])\n",
    "english_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import ISO 3 codes alongside ISO 2 codes in order to merge with dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd51bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_three_url = \"https://www.iban.com/country-codes\"\n",
    "\n",
    "iso_three_import = pd.read_html(iso_three_url)\n",
    "iso_three_import = iso_three_import[0].copy()\n",
    "iso_three_import = iso_three_import[  ['Alpha-2 code','Alpha-3 code']  ]\n",
    "iso_three_import = iso_three_import.rename(columns={'Alpha-2 code':'iso_code','Alpha-3 code':'iso3_code'})\n",
    "iso_three_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e983e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge and replace iso 2 with iso 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataframe = english_dataframe.merge(iso_three_import)\n",
    "english_dataframe = english_dataframe.drop(columns={'iso_code'})\n",
    "english_dataframe = english_dataframe.rename(columns={'iso3_code':'iso_code'}).copy()\n",
    "english_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1439906",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an 'id' column for a primary key for postGRES\n",
    "english_dataframe[\"id\"] = list(range(1, len(english_dataframe) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a5938",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "english_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns_order = ['id', 'en', 'iso_code', 'Year', 'WRI',\n",
    "                     'Exposure', 'Vulnerability', 'Susceptibility', 'Lack of Coping Capabilities',\n",
    "                     ' Lack of Adaptive Capacities', 'Exposure Category',\n",
    "                     'WRI Category', 'Vulnerability Category', 'Susceptibility Category']\n",
    "\n",
    "english_dataframe = english_dataframe[new_columns_order]\n",
    "english_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = ['id','country_name','iso_code','year','wri','exposure','vulnerability','susceptibility','coping_inability',\n",
    "     'adaptive_inability','wri_category','exposure_category','vulnerability_category','susceptibility_category']\n",
    "\n",
    "database_insert_df  = english_dataframe.copy()\n",
    "database_insert_df.set_axis(rename, axis=1,inplace=True)\n",
    "database_insert_df.to_csv('static/data/english_dataset.csv',index=False)\n",
    "database_insert_csv = pd.read_csv('static/data/english_dataset.csv')\n",
    "database_insert_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c70071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a1f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
