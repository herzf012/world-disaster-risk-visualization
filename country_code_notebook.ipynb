{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urlparse, urldefrag, parse_qs\n",
    "import pprint\n",
    "\n",
    "#API key pulled from a config.py in format of `prog_search_key` =  \"your_key_here\" \n",
    "from config import prog_search_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761456e7",
   "metadata": {},
   "source": [
    "## Attempt using country_codes_combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_csv = pd.read_csv('data/country_codes_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599aefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_csv = country_codes_csv[ ['alpha2','de','en'] ]\n",
    "country_codes_df = country_codes_csv.copy()\n",
    "country_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f720e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "world_risk_index_csv = pd.read_csv('data/world_risk_index.csv')\n",
    "world_risk_index_csv['Region'][1858]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ccddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Rows in country_codes_df: {len(country_codes_df.index)}')\n",
    "print(f'Rows in country_codes_df: {len(world_risk_index_csv.index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_en = world_risk_index_csv.merge(country_codes_df, how='left', left_on='Region', right_on='en')\n",
    "merged_df_de = world_risk_index_csv.merge(country_codes_df, how='left', left_on='Region', right_on='de')\n",
    "merged_all = pd.concat([merged_df_en, merged_df_de])\n",
    "merged_dropped = merged_all.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_world = merged_dropped.merge(world_risk_index_csv,how='right')\n",
    "merged_final = merged_world.drop_duplicates(ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad739c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final[merged_final['alpha2'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c73c7",
   "metadata": {},
   "source": [
    "## Attempt at Pulling From Additional Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0568da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_url = \"https://www.oenb.at/Statistik/Klassifikationen/ISO-Codes/ISO-Code-Verzeichnis-fuer-Laender--und-Waehrungscodes.html\"\n",
    "ger_codes = pd.read_html(ger_url)\n",
    "ger_code_draft = ger_codes[0].copy()\n",
    "ger_code_df = ger_code_draft[ ['Land','ISO-Code (Land)'] ]\n",
    "ger_code_df = ger_code_df.fillna(\"\").copy()\n",
    "ger_code_df_clean =  ger_code_df.loc[ger_code_df['ISO-Code (Land)']!='一一一']\n",
    "ger_code_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa0555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "world_risk_index_csv.merge(ger_code_df_clean, how='left', left_on='Region', right_on='Land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eac314",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cloford.com/resources/codes/index.htm\"\n",
    "\n",
    "country_code_import = pd.read_html(url)\n",
    "country_code_draft = country_code_import[3].copy()\n",
    "country_code_df = country_code_draft[  ['Country','ISO (2)','Continent','Region','Capital' ]  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_url = \"https://www.oenb.at/Statistik/Klassifikationen/ISO-Codes/ISO-Code-Verzeichnis-fuer-Laender--und-Waehrungscodes.html\"\n",
    "ger_codes = pd.read_html(ger_url)\n",
    "ger_code_draft = ger_codes[0].copy()\n",
    "ger_code_df = ger_code_draft[ ['Land','ISO-Code (Land)'] ]\n",
    "ger_code_df = ger_code_df.fillna(\"\").copy()\n",
    "ger_code_df_clean =  ger_code_df.loc[ger_code_df['ISO-Code (Land)']!='一一一']\n",
    "ger_code_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52889a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_import_codes = ger_code_df_clean.merge(country_code_df, left_on='ISO-Code (Land)', right_on='ISO (2)').copy()\n",
    "merged_import_codes_rename= merged_import_codes.rename(columns={'Region':'Area'}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_import_de = world_risk_index_csv.merge(merged_import_codes_rename, how='left', left_on='Region', right_on='Land')\n",
    "merged_df_import_en = world_risk_index_csv.merge(merged_import_codes_rename, how='left', left_on='Region', right_on='Country')\n",
    "import_merged_all = pd.concat([merged_df_import_en, merged_df_import_de])\n",
    "import_merged_dropped = import_merged_all.dropna().copy()\n",
    "import_merged_world = import_merged_dropped.merge(world_risk_index_csv,how='right')\n",
    "import_merged_final = import_merged_world.drop_duplicates(ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25fee4",
   "metadata": {},
   "source": [
    "## Attempt to finish cleaning by using custom google search API\n",
    "\n",
    "Finally, after trying to use two different sources for screening German names I found out that the original data input from the original dataset source was inconsistent. Following is how I solved this problem using Google's Custom Search API. \n",
    "\n",
    "Fortunately I was able to cut the amount of inconsistent naming conventions down to 33 unique countries. This fits within the Custom Search API's 100 free daily search limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96b395",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import_csv_merge = import_merged_final.merge(merged_final,how='outer').copy()\n",
    "iso_codes_df = import_csv_merge[ ['ISO (2)','alpha2'] ]\n",
    "import_csv_merge['iso_code'] = iso_codes_df.bfill(axis=1).iloc[:, 0]\n",
    "null_codes = import_csv_merge[import_csv_merge['iso_code'].isnull()].copy()\n",
    "null_codes['Region'] = null_codes['Region'].drop_duplicates().copy()\n",
    "null_codes = null_codes[null_codes['Region'].notna()]\n",
    "null_codes['Region'] = null_codes['Region'].str.replace('\\d+', '')\n",
    "null_codes = null_codes.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_codes_list = []\n",
    "null_codes_list = null_codes['Region'].tolist()\n",
    "null_codes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://www.googleapis.com/customsearch/v1/siterestrict\"\n",
    "cx = \"d3772df2249924485\"\n",
    "key = prog_search_key\n",
    "num = 1\n",
    "query_url = (f\"{api_url}?cx={cx}&key={key}&num={num}&q=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49489eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645b93e",
   "metadata": {},
   "source": [
    "# API Call \n",
    "## Please do not try to run this cell. I have set it to read-only. \n",
    "\n",
    "I have also commented it out as it should ONLY be used by Jacob McManaman, or by someone who knows what they are doing (or who is aware that *thinking* they know what they are doing can easily have consequences) and has willingly set up their Google API key for use with Google's Custom Search API. Someone who has done so must also have acknowledged that there is only 100 searches per day. Thoughtfully, this call will only run 33 searches.\n",
    "\n",
    "API aside, running this cell will reset the `request_list` list which *can* be something incredibly annoying. I believe I have taken steps to circumvent any accidents, but in the event I have not taken enough precaution, should someone go through the effort to change the cell from read-only and runs the cell frivolously, you will make the writer of this markdown doomingly sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae8e6d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# request_list= []\n",
    "# for country in null_codes_list:\n",
    "#     counter = counter + 1\n",
    "#     query = requests.get(f\"{query_url}{country}iso code\").json()\n",
    "#     print(f\"Search Request {counter} of {len(null_codes_list)} : {country}\")\n",
    "#     request_list.append(query)\n",
    "#     time.sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee7e30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(request_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae2d3d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_test_url = request_list[0]['items'][0]['link']\n",
    "split_list = test_test_url.split(':')\n",
    "split_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db533b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_code_list = []\n",
    "\n",
    "for request in range(len(request_list)):\n",
    "    request_url = request_list[request]['items'][0]['link']\n",
    "    url_split_list = request_url.split(':')\n",
    "    request_code_list.append(url_split_list[2])\n",
    "request_code_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49f75045",
   "metadata": {},
   "source": [
    "### Reasons for and Mechanics of the API Call:\n",
    "Originally, I had hoped that there was consistency with the original Dataset. I was very wrong and the German country/region names deviated from convention. Thankfully I was able to clean 98% (1884/1917) of the German region names using two external sources. \n",
    "\n",
    "I found that I could just google the final 2% (33) country names and google would correct the search to produce a country code provided from `de.wikipedia.org/`. A useful tool google provides is the ability to filter by website, through a **site:`www.example.com`** query, or by creating a [Programmable Search Engine](https://programmablesearchengine.google.com/about/). This programable enginge can then be utilized by [Google's Custom Search API](https://developers.google.com/custom-search/v1/overview). Limited by 100 free searches a day, this project is very fortunate that only 33 of the data needed this treatment. The overview of the API is as follows;\n",
    "\n",
    "`https://www.googleapis.com/customsearch/v1/siterestrict?cx=   &key=   &q=`\n",
    "\n",
    "Where `?cx=` is the engine ID that is referenced for the search, the `&key=` is the API key that is used to make the call, and `&q=` is the query. \n",
    "\n",
    "And so this API call utilizes a programmable engine set to specifically filter websites by `de.wikipedia.org/`. While other websites did populate, since a call needed to be made individually for each erroneous data , the german wikipedia was preferable since its results returned the single country/regions information page, while others returned a table with every other country code. Organically, the search would look something like this:\n",
    "\n",
    "![title](data/images/organic_search.png)\n",
    "\n",
    "Thanks to Google, any sort of cleaning of poorly inputed data is done for us by these request. It's just up to us to clean the resulting request results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeeed1a",
   "metadata": {},
   "source": [
    "## JSON cleaning\n",
    "\n",
    "Once the API call is done, the resulting JSON is sent to a list and that list is cleaned in this code. As of now it still needs some polishing I believe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_codes = []\n",
    "filtered_list = []\n",
    "filtered_codes = []\n",
    "fragment_list = []\n",
    "parsed_list = []\n",
    "filtered_codes = []\n",
    "\n",
    "for request in range(len(request_list)):\n",
    "        searched_codes.append(request_list[request]['items'][0]['link'])\n",
    "        parsed_list.append(urldefrag(searched_codes[request]).fragment)\n",
    "        frag_split = parsed_list[request].split(':')\n",
    "        filtered_codes.append(frag_split[])\n",
    "        \n",
    "filtered_codes_df = pd.DataFrame(filtered_codes)\n",
    "filtered_codes_df['Country'] = null_codes['Region']\n",
    "filtered_codes_df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc8899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15544f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_url = urlparse(filtered_url)\n",
    "# frag = urldefrag(filtered_url).fragment\n",
    "# frag_split = frag.split(':')\n",
    "# filtered_codes = frag_split[3]\n",
    "# filtered_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searched_codes = []\n",
    "# filtered_list = []\n",
    "# searched_codes.append(query)\n",
    "# searched_codes[0]['items'][0]['title']\n",
    "# filtered_list.append(searched_codes[0]['items'][0]['link'])\n",
    "# filtered_url = \"https://www.iso.org/obp/ui/#iso:code:3166:MD\"\n",
    "# parsed_url = urlparse(filtered_url)\n",
    "# frag = urldefrag(filtered_url).fragment\n",
    "# frag_split = frag.split(':')\n",
    "# filtered_codes = frag_split[3]\n",
    "# filtered_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bb13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
